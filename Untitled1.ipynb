{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMc5L3GAHBWfIi7lFO5Mpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakdj/mlearning/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX_H2eBqZGCP"
      },
      "source": [
        "!pip install JPype1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEOrHEhyZG6v"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTZ2WYwYZOl3"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import konlpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAIPhA-aCVX"
      },
      "source": [
        "tf.random.set_seed(777) #하이퍼파라미터 튜닝을 위해 실행시 마다 변수가 같은 초기값 가지게 하기\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7a-36EbbS2B"
      },
      "source": [
        "## 시작"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyLio1y3bVrJ"
      },
      "source": [
        "### 텍스트 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mea3ufTkaFS-"
      },
      "source": [
        "# 데이터 타입\n",
        "ENCODER_INPUT  = 0\n",
        "DECODER_INPUT  = 1\n",
        "DECODER_OUTPUT = 2\n",
        "\n",
        "# 태그 단어\n",
        "PADDING = \"<PADDING>\"   # 패딩\n",
        "START = \"<START>\"     # 시작\n",
        "END = \"<END>\"       # 끝\n",
        "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
        "\n",
        "# 한 문장에서 단어 시퀀스의 최대 개수\n",
        "sequence_length = 30\n",
        "\n",
        "# 태그 인덱스\n",
        "PADDING_INDEX = 0\n",
        "START_INDEX = 1\n",
        "END_INDEX = 2\n",
        "OOV_INDEX = 3\n",
        "\n",
        "def clean_korean_documents_simple_version(documents):\n",
        "    #텍스트 정제 (HTML 태그 제거)\n",
        "    for i, document in enumerate(documents):\n",
        "        document = BeautifulSoup(document, 'html.parser').text \n",
        "        #print(document) #스토리가 진짜 너무 노잼\n",
        "        documents[i] = document\n",
        "\n",
        "    #텍스트 정제 (특수기호 제거)\n",
        "    for i, document in enumerate(documents):\n",
        "        document = re.sub(r'[^ ㄱ-ㅣ가-힣]', '', document) #특수기호 제거, 정규 표현식\n",
        "        #print(document) stale and uninspired\n",
        "        documents[i] = document\n",
        "\n",
        "    #텍스트 정제 (형태소 추출)\n",
        "    for i, document in enumerate(documents):\n",
        "        okt = konlpy.tag.Okt()\n",
        "        clean_words = []\n",
        "        for word in okt.morphs(document): \n",
        "            clean_words.append(word)\n",
        "        #print(clean_words) #['스토리', '진짜', '노잼']\n",
        "        document = ' '.join(clean_words)\n",
        "        #print(document) #스토리 진짜 노잼\n",
        "        documents[i] = document\n",
        "\n",
        "    return documents"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmvt06MIaKzu"
      },
      "source": [
        "# 문장을 인덱스로 변환\n",
        "def convert_word_to_index(documents, word_to_index, document_usage): \n",
        "    documents_index = []\n",
        "\n",
        "    # 모든 문장에 대해서 반복\n",
        "    for document in documents:\n",
        "        document_index = []\n",
        "\n",
        "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
        "        if document_usage == DECODER_INPUT:\n",
        "            document_index.append(word_to_index[START])\n",
        "\n",
        "        # 문장의 단어들을 띄어쓰기로 분리\n",
        "        for word in document.split():\n",
        "            if word_to_index.get(word) is not None:\n",
        "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
        "                document_index.append(word_to_index[word])\n",
        "            else:\n",
        "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
        "                document_index.append(word_to_index[OOV])\n",
        "\n",
        "        # 최대 길이 검사\n",
        "        if document_usage == DECODER_OUTPUT:\n",
        "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
        "            if len(document_index) >= sequence_length:\n",
        "                document_index = document_index[:sequence_length-1] + [word_to_index[END]]\n",
        "            else:\n",
        "                document_index += [word_to_index[END]]\n",
        "        else:\n",
        "            if len(document_index) > sequence_length:\n",
        "                document_index = document_index[:sequence_length]\n",
        "\n",
        "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
        "        document_index += [word_to_index[PADDING]] * (sequence_length - len(document_index))\n",
        "\n",
        "        # 문장의 인덱스 배열을 추가\n",
        "        documents_index.append(document_index)\n",
        "\n",
        "    return np.asarray(documents_index)\n",
        "\n",
        "# 인덱스를 문장으로 변환\n",
        "def convert_index_to_word(indexs, index_to_word): \n",
        "    document = ''\n",
        "\n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == END_INDEX:\n",
        "            # 종료 인덱스면 중지\n",
        "            break;\n",
        "        if index_to_word.get(index) is not None:\n",
        "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
        "            document += index_to_word[index]\n",
        "        else:\n",
        "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
        "            document += index_to_word[OOV_INDEX]\n",
        "\n",
        "        # 빈칸 추가\n",
        "        document += ' '\n",
        "\n",
        "    return document    \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_m2-0tZbQ18"
      },
      "source": [
        "### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co5WN1naaNXm",
        "outputId": "175fdfa3-86d5-41ea-f4cf-8494947deb23"
      },
      "source": [
        "\n",
        "##########데이터 로드\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/deepseasw/seq2seq_chatbot/master/dataset/chatbot/ChatbotData.csv')\n",
        "\n",
        "##########데이터 분석\n",
        "\n",
        "##########데이터 전처리\n",
        "\n",
        "question = df['Q']\n",
        "answer = df['A']\n",
        "question = question.to_numpy()\n",
        "answer = answer.to_numpy()\n",
        "question = question[:100] #데이터를 100개로 제한\n",
        "answer = answer[:100]\n",
        "print(question[:3]) #['12시 땡!', '1지망 학교 떨어졌어']\n",
        "print(answer[:3]) #['하루가 또 가네요.', '위로해 드립니다.']\n",
        "\n",
        "# 형태소분석 수행\n",
        "question = clean_korean_documents_simple_version(question)\n",
        "answer = clean_korean_documents_simple_version(answer)\n",
        "print(question[:3]) #['12시 땡', '1 지망 학교 떨어졌어']\n",
        "print(answer[:3]) #['하루 가 또 가네요', '위로 해 드립니다']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡!' '1지망 학교 떨어졌어' '3박4일 놀러가고 싶다']\n",
            "['하루가 또 가네요.' '위로해 드립니다.' '여행은 언제나 좋죠.']\n",
            "['시 땡' '지망 학교 떨어졌어' '박일 놀러 가고 싶다']\n",
            "['하루 가 또 가네요' '위로 해 드립니다' '여행 은 언제나 좋죠']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4wRFwW_bNp3"
      },
      "source": [
        "#### 질문, 대답 문장들 합치는 구간"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mm-To1IaQ73"
      },
      "source": [
        "# 질문과 대답 문장들을 하나로 합침\n",
        "documents = []\n",
        "documents.extend(question)\n",
        "documents.extend(answer)\n",
        "# 단어들의 배열 생성\n",
        "words = []\n",
        "for document in documents:\n",
        "    for word in document.split():\n",
        "        words.append(word)\n",
        "# 길이가 0인 단어는 삭제\n",
        "words = [word for word in words if len(word) > 0]\n",
        "# 중복된 단어 삭제\n",
        "words = list(set(words))\n",
        "# 제일 앞에 태그 단어 삽입\n",
        "words = [PADDING, START, END, OOV] + words\n",
        "print(words[:10]) #['<PADDING>', '<START>', '<END>', '<OOV>', '으로', '아세요', '다음', '모두', '정말', '그런거니']\n",
        "vocab_size = len(words)\n",
        "# 단어와 인덱스의 딕셔너리 생성\n",
        "word_to_index = {word: index for index, word in enumerate(words)} #단어 -> 인덱스, 문장을 인덱스로 변환하여 모델 입력으로 사용\n",
        "index_to_word = {index: word for index, word in enumerate(words)} #인덱스 -> 단어, 모델의 예측 결과인 인덱스를 문장으로 변환시 사용\n",
        "\n",
        "# 인코더 입력 인덱스 변환\n",
        "x_encoder = convert_word_to_index(question, word_to_index, ENCODER_INPUT)\n",
        "print(x_encoder[:3]) \n",
        "'''\n",
        "[[140 413   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
        " [336  49  61 223   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]]\n",
        "'''\n",
        "# 디코더 입력 인덱스 변환 (START ~)\n",
        "x_decoder = convert_word_to_index(answer, word_to_index, DECODER_INPUT)\n",
        "print(x_decoder[:3]) \n",
        "'''\n",
        "[[  1 356 257 343 180   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
        " [  1 106 187 289   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]]\n",
        "'''\n",
        "# 디코더 목표 인덱스 변환 (~ END)\n",
        "y_decoder = convert_word_to_index(answer, word_to_index, DECODER_OUTPUT)\n",
        "print(y_decoder[:3]) \n",
        "'''\n",
        "[[356 257 343 180   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
        " [106 187 289   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0   0   0   0   0   0   0   0   0   0   0]]\n",
        "'''\n",
        "#원핫 인코딩\n",
        "y_decoder = tf.keras.utils.to_categorical(y_decoder, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTtWfvwZbKQJ"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4hAMsxQabI2"
      },
      "source": [
        "##########모델 생성\n",
        "\n",
        "encoder_input = tf.keras.layers.Input(shape=(None,)) #입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
        "decoder_input = tf.keras.layers.Input(shape=(None,)) #목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
        "net = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100)(encoder_input)\n",
        "net, state_h, state_c = tf.keras.layers.LSTM(units=128, return_sequences=True, return_state=True)(net)\n",
        "#net, state_h, state_c = tf.keras.layers.LSTM(units=128, return_sequences=True, return_state=True, dropout=0.1, recurrent_dropout=0.5)(net)\n",
        "net = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100)(decoder_input)\n",
        "net, state_h, state_c = tf.keras.layers.LSTM(units=128, return_sequences=True, return_state=True)(net, initial_state=[state_h, state_c]) #initial_state를 인코더의 상태로 초기화\n",
        "#net, state_h, state_c = tf.keras.layers.LSTM(units=128, return_sequences=True, return_state=True, dropout=0.1, recurrent_dropout=0.5)(net, initial_state=[state_h, state_c]) #initial_state를 인코더의 상태로 초기화\n",
        "net = tf.keras.layers.Dense(units=vocab_size, activation='softmax')(net) #단어의 개수만큼 노드의 개수를 설정해 원핫 형식으로 각 단어 인덱스를 출력\n",
        "model = tf.keras.models.Model([encoder_input, decoder_input], net)\n",
        "\n",
        "model.summary()\n",
        "'''\n",
        "Model: \"model\"\n",
        "__________________________________________________________________________________________________\n",
        "Layer (type)                    Output Shape         Param #     Connected to                     \n",
        "==================================================================================================\n",
        "input_1 (InputLayer)            [(None, None)]       0                                            \n",
        "__________________________________________________________________________________________________\n",
        "input_2 (InputLayer)            [(None, None)]       0                                            \n",
        "__________________________________________________________________________________________________\n",
        "embedding (Embedding)           (None, None, 100)    45400       input_1[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "embedding_1 (Embedding)         (None, None, 100)    45400       input_2[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "lstm (LSTM)                     [(None, None, 128),  117248      embedding[0][0]                  \n",
        "__________________________________________________________________________________________________\n",
        "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[0][0]                \n",
        "                                                                 lstm[0][1]                       \n",
        "                                                                 lstm[0][2]                       \n",
        "__________________________________________________________________________________________________\n",
        "dense (Dense)                   (None, None, 454)    58566       lstm_1[0][0]                     \n",
        "==================================================================================================\n",
        "Total params: 383,862\n",
        "Trainable params: 383,862\n",
        "Non-trainable params: 0\n",
        "__________________________________________________________________________________________________\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yr-ERjsbbQw"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg_UPE_2aiYc"
      },
      "source": [
        "##########모델 학습 및 검증\n",
        "\n",
        "#인코더\n",
        "encoder_input = model.input[0]\n",
        "net = model.layers[2](encoder_input)\n",
        "net, state_h, state_c = model.layers[4](net)\n",
        "encoder_model = tf.keras.models.Model(encoder_input, [state_h, state_c])\n",
        "\n",
        "encoder_model.summary()\n",
        "'''\n",
        "Model: \"model_1\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input_1 (InputLayer)         [(None, None)]            0         \n",
        "_________________________________________________________________\n",
        "embedding (Embedding)        (None, None, 100)         45400     \n",
        "_________________________________________________________________\n",
        "lstm (LSTM)                  [(None, None, 128), (None 117248    \n",
        "=================================================================\n",
        "Total params: 162,648\n",
        "Trainable params: 162,648\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "'''\n",
        "\n",
        "#디코더\n",
        "decoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "state_h_input = tf.keras.layers.Input(shape=(128,))\n",
        "state_c_input = tf.keras.layers.Input(shape=(128,))   \n",
        "net = model.layers[-4](decoder_input)\n",
        "net, state_h, state_c = model.layers[-2](net, initial_state=[state_h_input, state_c_input])\n",
        "net = model.layers[-1](net)\n",
        "decoder_model = tf.keras.models.Model([decoder_input, state_h_input, state_c_input], [net, state_h, state_c])\n",
        "\n",
        "decoder_model.summary()\n",
        "'''\n",
        "Model: \"model_2\"\n",
        "__________________________________________________________________________________________________\n",
        "Layer (type)                    Output Shape         Param #     Connected to                     \n",
        "==================================================================================================\n",
        "input_3 (InputLayer)            [(None, None)]       0                                            \n",
        "__________________________________________________________________________________________________\n",
        "embedding_1 (Embedding)         (None, None, 100)    45400       input_3[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
        "__________________________________________________________________________________________________\n",
        "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
        "__________________________________________________________________________________________________\n",
        "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[1][0]                \n",
        "                                                                 input_4[0][0]                    \n",
        "                                                                 input_5[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense (Dense)                   (None, None, 454)    58566       lstm_1[1][0]                     \n",
        "==================================================================================================\n",
        "Total params: 221,214\n",
        "Trainable params: 221,214\n",
        "Non-trainable params: 0\n",
        "__________________________________________________________________________________________________\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPA01wLakid"
      },
      "source": [
        "#내장 루프\n",
        "def on_epoch_end(epoch, logs):\n",
        "    x_encoder = np.array([\n",
        "        '3 박 4일 놀러 가고 싶다'\n",
        "    ])\n",
        "\n",
        "    x_encoder = clean_korean_documents_simple_version(x_encoder)\n",
        "    x_encoder = convert_word_to_index(x_encoder, word_to_index, ENCODER_INPUT)\n",
        "    print(x_encoder) #\n",
        "    print(len(x_encoder[0])) #\n",
        "\n",
        "    x_decoder = np.array([\n",
        "        '여행 은 언제나 좋죠'\n",
        "    ])\n",
        "\n",
        "    x_decoder = clean_korean_documents_simple_version(x_decoder)\n",
        "    x_decoder = convert_word_to_index(x_decoder, word_to_index, DECODER_INPUT)\n",
        "    print(x_decoder) #\n",
        "    print(len(x_decoder[0])) #\n",
        "\n",
        "    y_predict = model.predict([x_encoder, x_decoder])\n",
        "    #print(y_predict.shape) #(1, 30, 454)\n",
        "    indexs = y_predict.argmax(axis=2)\n",
        "    #print(indexs) #[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
        "\n",
        "    # 인덱스를 문장으로 변환\n",
        "    text = convert_index_to_word(indexs[0], index_to_word)\n",
        "    print()\n",
        "    print('3 박 4일 놀러 가고 싶다 ->', text) #<PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> \n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) #optimizer='rmsprop' 더 좋은 성능\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=[tf.keras.metrics.Accuracy()])\n",
        "\n",
        "model.fit([x_encoder, x_decoder], y_decoder, epochs=2000, validation_split=0.3, callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSTUwB91bf-k"
      },
      "source": [
        "### 모델 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ql2M6Rlan69",
        "outputId": "c5dc0b26-e6cc-45eb-86c6-ff95446a724e"
      },
      "source": [
        "##########모델 예측\n",
        "\n",
        "print('인공지능 챗봇')\n",
        "print('인공지능 챗봇과 대화를 합니다.')\n",
        "\n",
        "x_encoder = np.array([\n",
        "    '3박4일 놀러가고 싶다'\n",
        "])\n",
        "\n",
        "x_encoder = clean_korean_documents_simple_version(x_encoder)\n",
        "x_encoder = convert_word_to_index(x_encoder, word_to_index, ENCODER_INPUT)\n",
        "#print(x_encoder) #[[209 201 271  70 219 113   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
        "#print(len(x_encoder[0])) #30\n",
        "\n",
        "# 입력을 인코더에 넣어 마지막 상태 구함\n",
        "state_h, state_c = encoder_model.predict(x_encoder)\n",
        "#print(state_h.shape) #(1, 128)\n",
        "#print(state_c.shape) #(1, 128)\n",
        "\n",
        "# 인덱스 초기화\n",
        "indexs = []\n",
        "\n",
        "# 목표 시퀀스 초기화\n",
        "x_decoder = np.zeros((1, 1))\n",
        "# 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
        "x_decoder[0, 0] = START_INDEX\n",
        "#print(x_decoder) #[[1.]]\n",
        "\n",
        "# 디코더로 현재 타임 스텝 출력 구함\n",
        "# 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
        "# 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
        "y_predict, state_h, state_c = decoder_model.predict([x_decoder, state_h, state_c])\n",
        "#print(y_predict.shape) #(1, 1, 454)\n",
        "#print(state_h.shape) #(1, 128)\n",
        "#print(state_c.shape) #(1, 128)\n",
        "# 결과의 원핫인코딩 형식을 인덱스로 변환\n",
        "#print(y_predict[0, 0, :].shape) #(454,)\n",
        "index = y_predict[0, 0, :].argmax()\n",
        "#print(index) #0\n",
        "indexs.append(index)\n",
        "\n",
        "# 디코더 타임 스텝 반복\n",
        "while index != END_INDEX:\n",
        "    # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
        "    x_decoder = np.zeros((1, 1))\n",
        "    x_decoder[0, 0] = index\n",
        "    #print(x_decoder) #[[0.]]\n",
        "\n",
        "    # 디코더로 현재 타임 스텝 출력 구함\n",
        "    # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
        "    # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
        "    y_predict, state_h, state_c = decoder_model.predict([x_decoder, state_h, state_c])\n",
        "    #print(y_predict.shape) #(1, 1, 454)\n",
        "    #print(state_h.shape) #(1, 128)\n",
        "    #print(state_c.shape) #(1, 128)\n",
        "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
        "    #print(y_predict[0, 0, :].shape) #(454,)\n",
        "    index = y_predict[0, 0, :].argmax() \n",
        "    #print(index) #0\n",
        "    indexs.append(index)\n",
        "\n",
        "    # 종료 검사\n",
        "    if len(indexs) >= sequence_length:\n",
        "        break\n",
        "\n",
        "# 인덱스를 문장으로 변환\n",
        "#print(indexs) #[194, 257, 379, 2] \n",
        "text = convert_index_to_word(indexs, index_to_word)\n",
        "print(text) #저 도 요 "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인공지능 챗봇\n",
            "인공지능 챗봇과 대화를 합니다.\n",
            "여행 은 언제나 좋죠 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya4mI5bcbjrv"
      },
      "source": [
        "#### 참고)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVPHyjsUbn7f"
      },
      "source": [
        "encoder_model = Model(inputs=question_input, outputs=[state_h, state_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvJCpgBLbp94"
      },
      "source": [
        "### 모델 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "cJXWZ4xSb8sN",
        "outputId": "7808b91a-ec4e-44ad-c0eb-ec4c44ad18d2"
      },
      "source": [
        "##########모델 학습 및 검증\n",
        "\n",
        "# 인덱스를 문장으로 변환\n",
        "def convert_index_to_text(indexs, index_to_word): \n",
        "\n",
        "    sentence = ''\n",
        "\n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == END_INDEX:\n",
        "            # 종료 인덱스면 중지\n",
        "            break;\n",
        "        if index_to_word.get(index) is not None:\n",
        "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
        "            sentence += index_to_word[index]\n",
        "        else:\n",
        "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
        "            sentence.extend([index_to_word[OOV_INDEX]])\n",
        "\n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "\n",
        "    return sentence\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) #optimizer='rmsprop' 더 좋은 성능\n",
        "#model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=[tf.keras.metrics.Accuracy()])\n",
        "\n",
        "# 에폭 반복\n",
        "for epoch in range(20):\n",
        "    print('Total Epoch :', epoch + 1)\n",
        "\n",
        "    # 훈련 시작\n",
        "    history = model.fit([x_encoder, x_decoder], y_decoder, epochs=100, verbose=0)\n",
        "    print(history.history)\n",
        "    # 정확도와 손실 출력\n",
        "    print('accuracy :', history.history['accuracy'][-1])\n",
        "    print('loss :', history.history['loss'][-1])\n",
        "\n",
        "    # 문장 예측 테스트\n",
        "    # (3 박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)\n",
        "    input_encoder = x_encoder[2].reshape(1, x_encoder[2].shape[0])\n",
        "    input_decoder = x_decoder[2].reshape(1, x_decoder[2].shape[0])\n",
        "    results = model.predict([input_encoder, input_decoder])\n",
        "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
        "    # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
        "    indexs = np.argmax(results[0], 1) \n",
        "\n",
        "    # 인덱스를 문장으로 변환\n",
        "    sentence = convert_index_to_text(indexs, index_to_word)\n",
        "    print(sentence)\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epoch : 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-39f71f18ebbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# 훈련 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_decoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 정확도와 손실 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 1\n  y sizes: 100\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    }
  ]
}